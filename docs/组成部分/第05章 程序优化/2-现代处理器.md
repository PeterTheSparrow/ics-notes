---
title: 第二节 现代处理器
sidebar_position: 2
---

import OfficePreview from '@site/src/components/OfficePreview/index';

<OfficePreview place = "/ppt/2-8-mio.ppt"/>

## 第二节 现代处理器

> 了解处理器是怎么执行我们的汇编代码，对应课本的5.7

### 一、处理器特点

#### 1）概述

- Superscalar：超标量体系结构，在每一个时钟周期里面可以执行多条指令，实现指令级别的并行
- Out-of-order execution：乱序执行，指令执行的顺序和指令在汇编代码的部分不一样。（因为我们一次取出了多条指令，这就可能会导致某个靠后的指令跑在了前面）
- 因此，现代的处理器可以分为两个部分：
  - ICU：指令控制单元，负责从内存里面读取一系列的指令。同时他可能会做一些拆分，把复杂的指令可能拆成粒度更细的指令。
  - EU：指令执行单元，复杂执行这些指令
- 具体说来如下所示：

![截屏2023-03-27 21.20.57](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2021.20.57.png)

- Instruction Cache是一个特殊的内存（比内存更快的一个存储部分），包括最近需要访问的指令（假如每次读取指令都从内存的话，太慢了）
- FetchControl就提前把后面可能要执行的指令拿出来
- InstructionDecode就是把复杂的指令拆一下，可能拆成比较小的指令。比如读内存又写入的指令，就可以拆一下！
- 对于跳转的分支，会有一个Breach Predict，预测下一个指令会跳转到哪（是一个重要的核心组件）

#### 2）Register Renaming

> 现在问题来了，假如指令之间存在依赖？怎么办？这里就涉及到寄存器重命名!

##### a）读写问题

首先是读写依赖，假如上一个指令读某个寄存器，下一个指令写某个寄存器，那这个时候就必须区分顺序，读指令必须在前，写指令必须在后面。同样的道理，假如上一个指令写某个寄存器，下一个指令读取某个寄存器，也不能反过来。为了解决这个依赖的问题，解决方法就是寄存器重命名

假设我有这样一个指令：

```
addq $8, %rax;
# 然后会被CPU翻译为
addq $8, %rax.0;
```

- CPU内部会维护一张表，类似一个键值对的表，包含寄存器和他的Tag（key），还有寄存器的值（value）
- 假如一个指令进来之前，发现要读取的寄存器还没有被写入（也就是说已经被打上了Tag），这个指令就不会被放进去
- 但是如果发现这个指令要用的寄存器已经被写入了，那这个指令就可以进去了
- 这样就增加了并行的效果。

##### b）OUTPUT

假如两个指令都要写入到某个相同的寄存器，此外他们都在并行执行，我们也可以通过寄存器重命名的方法，消除了依赖关系，这样也增加了并行的可行性。

##### c）如何理解rename

- 假如我要依次执行下面的三个语句

```
x = x + data[0];
 \
  \
   \
x = x + data[1];
 \
  \
   \
x = x + data[2];
```

- 可以看到明显的依赖关系

```
# 解决方案
x1 = x0 + data[0];
x2 = x1 + data[1];
x3 = x2 + data[2];
```

- 通过打标签就完美搞定了！
- 补充：实现机制：CPU的内部有很多空闲的临时寄存器，可以用来存储这些。Tag通过一个RenameTable来维护。
- 在最终决定某个寄存器的值是什么之前，还不能修改寄存器，需要考虑RenameTable仔细考虑
- 只有当指令彻底结束了，才会根据RenameTable把寄存器的值真正的写入到寄存器

### 二、案例分析

还是回归我们上节课的例子：

```c
void combine4(vec_ptr v, data_t *dest)
{
  long i;
  long length = vec_length(v);
  data_t *data = get_vec_start(v);
  data_t x = IDENT;

  for (i = 0; i < length; i++)
    x = x OP data[i];
  *dest = x;
}
```

- 这个是它对应的汇编代码：

```
.L25:		 # Loop:
vmulsd (%rdx),%xmm0,%xmm0	 # t *= data[i]
addq $8, %rdx		 # Increment data+i
cmpq %rax,%rdx		 # Comp to data+len
jne .L25		 # if !=, goto Loop
```

- 这个是经过处理之后的，可以看到增加了Tag值，包括CC，同时还把一个复杂的指令拆成立简单的指令

```
load (%rdx.0)       # t.1
mulq t.1, %xmm0.0   # %xmm0.1
addq $8, %rdx.0     # %rdx.1
cmpq %rax, %rdx.1   # cc.1
jne-taken cc.1
```

- 有了Tag的机制之后，假如一个指令计算的结果放到了某个寄存器RDX，他会马上写入到Tag的键值对系统中，然后后面要用到的指令马上就可以看到这个结果了，提高了效率。
- 此外，如果一个指令对于`cmpq %rax, %rdx.1`对于寄存器没有修改效果，就不会维持Tag的标记！

### 三、Intel i7的处理单元

#### 1）处理单元介绍

1. Integer arithmetic, FP multiplication, integer and FP division, branches 
2. Integer arithmetic, FP addition, integer multiplication, FP multiplication 
3. Load, address computation 
4. Load, address computation 
5. Store 
6. Integer arithmetic(basic operations) 
7. Integer arithmetic, branches 
8. Store address computation 

- 对比下表格，如果发现Latency和Issue Time(Issue Time应该是每出现一个指令执行结束的时间，例如五级流水里面的每一个时钟周期就有一个指令从流水线里面出来，也就是吞吐量)不一样，那就说明存在流水线的设计
- Multiplication：消耗的周期3个，说明有三个流水线
- FloatMultiplication：浮点数乘法需要5个流水线

![image-20230327221735306](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/image-20230327221735306.png)

#### 2）ReOrder

- 如下图所示
- ReOrderBuffer：我们通过Decode、Renaming之后得到的指令就会被放在ReOrderBuffer，也就是指令的缓冲区域，这个缓冲区进入之后，顺序可能会发生变化（比如指令的前后顺序）
- 指令一旦放进去，顺序就不重要了！Dispatch会让哪个指令有资格进入执行单元，谁（哪个指令）有资格进去，谁就可以进去。（两个条件：一个是执行单元有空闲了，可以放指令进去了！另外一个条件是即将进入的指令所用到的操作数全部Ready了！用到的操作数不能还没有计算完，那就会出问题）满足条件之后，会把可以执行的指令拿出来
- 所以OoO就是简称（Out Of Order的执行）

![截屏2023-03-27 22.22.57](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.22.57.png)

- 指令执行完成之后，就直接放在DataCache？不能直接写！假如这个指令是个错误的或者不应该执行的指令，那这要是写了就会出现问题。所以OoO指令带来的问题也是可见一斑的

![截屏2023-03-27 22.30.28](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.30.28.png)

#### 2）原理分析

> 刚刚的那个表格里面我们看到Latency是1、3、3、5，下面我们来分析原理为什么。

再次回到我们之前的代码：


```
load (%rdx.0)       # t.1
mulq t.1, %xmm0.0   # %xmm0.1
addq $8, %rdx.0     # %rdx.1
cmpq %rax, %rdx.1   # cc.1
jne-taken cc.1
```

- 把程序的依赖的流图画出来！可以发现Load阶段和Add阶段两个步骤可以并行，而且他们在不同的单元执行，可以并行执行

![截屏2023-03-27 22.39.27](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.39.27.png)

- 删减掉不必要的（为什么，因为删除的cmp和jne两个的时延都很短！忽略掉小的），关键就是这三个操作。mul的延迟大概是5，而add的延迟大概是1。
- 最终的结果急速Mul可能半天没有动静，但是Load可以提前做啊！还有Add也可以提前做。
- （尽管Load需要依赖Add的结果，但是Load和Add的延迟都比较短

![截屏2023-03-27 22.44.20](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.44.20.png)

- 这样我们就可以画出Critical路径（关键路径）就是最左边的乘法的路径，他决定了主要的延迟的时间。

### 四、更多优化

#### 1）循环展开

- 原来n个循环，每个循环做一次，现在n/2个循环，每个循环做两个步骤
- 优点：原来是每次执行一次都会遇到上面的jmp啊之类的，现在每次执行两个运算操作，这两个运算操作甚至可以并行

```
void combine5(vec_ptr v, int *dest)
{
  int i;
  int length = vec_length(v);
  int limit = length - 2;
  data_t *data = get_vec_start(v);
  data_t acc = IDENT;

  /* combine 2 elements at a time */
  for (i = 0; i < limit; i+=2)
    acc = acc OPER data[i] OPER data[i+1];

  /* finish any remaining elements */
  for (; i < length; i++)
    acc = acc OPER data[i];
 
  *dest = acc ;
}
```

- 这样下来的程序流图

![截屏2023-03-27 22.53.17](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.53.17.png)

- 尽管对于浮点数效果没大改变，但是对于整数的加法，效果大大提高！
- 上一个Load了马上就可以Add！

![截屏2023-03-27 22.58.03](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.58.03-9929130.png)

- 优化结果如下所示！

![截屏2023-03-27 22.58.42](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2022.58.42.png)

#### 2）Multiple Accumulator

- 如下所示（分成奇数、偶数分别来求和）：

```c
void combine6(vec_ptr v, int *dest)
{
  int i;
  int length = vec_length(v), limit = length-1;
  data_t *data = get_vec_start(v);
  data_t acc0 = IDENT, acc1 = IDENT;

  /* combine 2 elements at a time */
  for (i = 0; i < limit; i+=2){
    acc0 = acc0 OPER data[i];
    acc1 = acc1 OPER data[i+1];
  }

  /* finish any remaining elements */
  for (; i < length; i++)
    acc0 = acc0 OPER data[i];
 
  *dest = acc0 OPER acc1;
}
```

- 现在就实现了两个乘法的并行处理

![截屏2023-03-27 23.02.42](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2023.02.42.png)

- 最终的实现效果如下所示。
- 只要并行足够多，ThroughPUT就可以达到0.5、1、1这样的效果

![截屏2023-03-27 23.03.28](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-03-27%2023.03.28.png)

### 五、限制因素

#### 1）两种对比

- independ优化比较高级，不关心平台，主要思想是减少开销比较高的操作，比如循环中的访问内存
  - 当我们要去检查这个字符串的长度的时候，我们可以在外面直接算出来这个值，然后把它作为 for 循环的这个值。但如果这个字符串同时在被修改，那你每一次去访问的时候可能是不一样的，不一样。
- 第二部分，我们介绍了 machine optimization。如果有多个硬件的时候，我们可以有一些优化来提升这个并行性，在指令级别的这个并行性。前提条件主要就是硬件它存在并行性，它有多个相同功能的这个器件，

#### 2）影响因素

- 第一个是硬件的资源，硬件资源除了 Aru 里面的这个做 Aru 的这个器件之外，还有寄存器。寄存器用完了之后，后面就只能用内存，性能就下降。寄存器不够用，这种情况我们称为 register spilling。
  - 例如我们使用累计变量的思路，假如我使用非常多的累计变量，超过了寄存器的数量，反而适得其反（因为要用到内存了！寄存器都用满了）
- 第二个问题是来自于 branch 的这个预测，CPU在面对Jmp预测的时候，如果预测错了，之前执行的就白执行了。（首先需要保证正确、然后考虑性能）这个是由 instruction 的这个 Ctrl unit 来完成的
  - 用 speculative execution，尽量让所有的器件都处于忙的状态，即使我不确定，那么我可以把两条路径都一块执行。当你发现猜错了之后，我要消除它产生的这个错误的影响。
  - 现在流水线越来越大胆，十几级的流水线，一旦判断出错可能执行了很多代码。统一的方案，就是**Retirement Unit**，register file 就本身就被包含在里面。指令执行的结果暂时放在这，不会真正的写到寄存器。
  - 对于这些指令放到 retirement unit 里面的结果会有两种情况，retire就是你这个结果被确定下来，应该写到寄存器了。 flash 就是因为错误指令，这些中间的结果。

#### 3）举例子

- 下面的代码是让交换ab数组的元素，数组b的元素更大

```cpp
void minmax1(int a[], int b[], int n)
{
  int i;
  for (i = 0; i < n; i++) {
    if (a[i] > b[i]) {
      int t = a[i];
      a[i] = b[i];
      b[i] = t;
    }
  }
}
```

- 上面的代码跟输入有很大的关系，如果输入的数据比较随机，可能会出现CPE比较大的情况。因为CPU在猜的时候，因为里面多了一层if判断，所以猜错的情况经常发生
- 所以修改之后：

```cpp
void minmax2(int a[],      int b[], int n)
{
  int i;
  for (i = 0; i < n; i++) {
    int min = a[i]<b[i]?a[i]:b[i];
    int max = a[i]<b[i]?b[i]:a[i];
    a[i] = min;
    b[i] = max;
  }
}
```

- 现在，就可以在比较稳定的时间内部完成之后

### 六、理解内存性能

#### 1）例子1

- 例如下面的一段遍历链表的代码

```cpp
typedef struct ELE {
	struct ELE *next ;
	int data ;
} list_ele, *list_ptr ;

long list_len(list_ptr ls) {
	long len = 0 ;
	while (ls) {
		len++ ;
		ls = ls->next;
	}  
	return len ;
}
```

- 对应的汇编代码如下

```
len in %rax, ls in %rdi
.L3:
	addq $1, %rax
	movq	(%rdi), %rdi
	testq  %rdi, %rdi
	jne	.L3
```

- 这个代码就产生了一个依赖`ls = ls->next;`，虽然loader器件每一个 cycle 都去做一次loader，但是由于这种依赖关系，必须要上一个指令执行完成后，这下一个指令才可以做下一次load
- 因此CPE就很高，大约需要到4
- 但是下面的代码Issue时间就是1.0了

```cpp
void array_clear(int *dest, int n) 
{
	int i;
	int limit = n-3;
	for (i = 0; i < limit; i++)
		dest[i] = 0;
}
```

#### 2）例子2

- 下面给出相同的代码，不同的输入情况下，性能的差别

```cpp
void write_read(long *src, 
     long *dest, long n)
{
	long cnt = n;
	long val = 0;

	while (cnt) {
		*dest = val;
		val = (*src)+1;
		cnt--;
	}
}
```

- 这个代码是读取两个地址，把源地址的数据经过一个处理之后，然后写到目标地址里面去！这个操作会反复执行。我们分为两个情况考虑；
  - 假如源地址SRC和DEST不相同，这个时候不存在依赖关系。上一次读取的SRC地址里面的值从来不会变，所以CPE就比较好效果
  - 假如SRC和DEST是同一个地址，这时候存在依赖关系。上一次读取的SRC地址里面的值经过处理之后又写回到原来的地址了！
- 前面介绍的这个 retirement unit 是只包含了寄存器之类，但是实际情况下对于内存的写入操作其实也是要暂时放到了retirement unit 里面的，

![截屏2023-04-07 11.35.12](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-04-07%2011.35.12.png)

- 所以对于这个 ICore7 来说，它这个执行是 parallel 的（甚至可以把那些没有依赖关系的指令乱序的去执行，乱序是可以执行，但是最终的结果是要有序提交），就是通过这个 store buffer 来完成的，就是你乱序执行可能产生了好多结果，但它最终的这个提交，表现在 retire 的这个顺序是要有序的
- 如下图所示

![截屏2023-04-07 11.39.26](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-04-07%2011.39.26.png)

-  loader 数据的时候，你就要判断是从 date cache 里面 loader 还是从这个 store unit 里面loader
  - store unit 里面有一些是不确定的，而有一些是确定的，但是你也不能马上写回
  - 可能是因为并发执行先执行了。你这个写回必须要按顺序写回，
  - 这时候的load的会分成两种，从 cache loader 和这个 store unit loader， store unit 可能会更新。
  - 总之，执行的时候可以随便乱序执行，但是写的时候（写的时候无非是写寄存器、写内存）必须顺序。我们的方案是先在 store 里面去缓存这些结果，然后顺序的去提交到这个 date cache 里面。这就是乱序执行顺序提交

- 具体分析循环体的语句，可以得到如下的代码：

```cpp
//inner-loop
  while (cnt) {
	 *dest = val;
 	 val = (*src)+1;
	 cnt--;
  }
```

```
movq %rax,(%rsi)
movq (%rdi), %rax
addq $1,%rax
subq $1,%rdx
jne loop
```

![截屏2023-04-07 13.41.26](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-04-07%2013.41.26.png)

- 如下图所示，左边的情况是不存在依赖关系的时候，
- `s_data`对应的是写操作`movq %rax,(%rsi)`，
- `load`对应的是加载内存`movq (%rdi), %rax`
- 后面的加法减法就好说了

![截屏2023-04-07 13.39.14](./2-%E7%8E%B0%E4%BB%A3%E5%A4%84%E7%90%86%E5%99%A8.assets/%E6%88%AA%E5%B1%8F2023-04-07%2013.39.14.png)

> 思考：为什么左边的关键路径是做sub？左边的不是有load、add、s_data吗
>
> - sub占用的时间其实是最长的！可能高达5个Cycle的延迟
> - Load操作可以放在上一步Add的时候同步做，反正不影响！
> - s_data写入的其实是上一个步骤Add之后的结果，所以可以放在下一个循环体里面Add的时候做
> - 这样一来，这两个步骤就基本上不需要消耗时间了，CPE的值理论就在1左右，实际测的1.3（因为可能考虑了首尾两头的情况，大一点很正常）

#### 3）如何高性能

- 使用好的编译器，添加`-o2`等，尽可能使用已有的工具优化，不需要了解代码
- 比如Visual Studio使用时，有Debug和Release版本，在生成代码的时候也有区别
- 从源代码优化，编写对于编译器友好的代码。比如内存引用、过程调用
- 然后考虑机器相关的优化：
  - 考虑指令级别的并行！
  - 避免不可预测的分支！
  - 缓存友好级的代码（后面讲完Cache之后）

### 七、Tuning

> 背景介绍：要分析一篇英文文章，里面单词（连续n个单词）出现的次数。

#### 1）步骤

- 把文章转化为小写字符
- 使用哈希函数的功能，避免查找时间太复杂
- 每次读取n个单词，然后在哈希表里面查找

#### 2）公式

- 根据**Amdahl’s Law**，只是优化局部，对于最终的影响其实是不大的。

$$
S = T_{old} / T_{new} = 1/[(1-a) + a/k]
$$

- 所以不要看到代码就去优化，否则优化效果微乎其微。需要用工具分析，把性能不好的瓶颈找到。

#### 3）GPROF

- 首先确保正确的编译程序，并开启相关的优化
- 然后使用典型的输入（因为我们知道输入对于性能效果影响也很大）

- 分析性能的瓶颈，然后找到花费时间最多的（或者执行次数多的，我们倾向执行次数多的优化！）
- 我们可以使用GPROF工具

```
$gcc -Og –pg prog.c –o prog 
```

- 你可以理解为他在编译的时候插入了一些输出`printf`的语句
- 然后运行程序，程序会输出一些运行的过程信息

```
$./prog file.txt
# Creates file gmon.out containing statistics
```

- 然后创建分析报告

```
$gprof prog > myreport
$cat myreport
```

- 例如下面的就是一个结果分析

```
 %      cumulative  self                         self         total
time    seconds     seconds  calls      s/call    s/call      name
97.58   203.66      203.66    1         203.66    203.66     sort_words
2.32    208.50      4.85     965027     0.00        0.00     find_ele_rec
0.14    208.81      0.30     12511031   0.00        0.00     Strlen
```

- name：函数的名字、self seconds代表在这个函数里面自身执行的时间，cumulative时间就是累计时间。因为函数包括嵌套递归之类。self seconds表示就是在自己函数的时间，累计的就是包括调用其他函数的所有的执行时间。calls代表调用了多少次数
- 分析的时候有时候用的平均，把调用的所有的时间除以调用的次数。避免计时带来的开销太大了

#### 4）优化实际举例

- 首先优化排序方法，发现快速排序，优化后从250变成4s了！
- 然后发现哈希表可能太小了，然后增大哈希表的容量，这样每个哈希表后面链接的单词数量就变少了！查找时间就更快了！
- 发现效果不明显，提高不多，那就怀疑可能哈希函数不好。实际证明哈希表后面的桶都没被用到！
- 于是改进哈希函数，通过移位亦或各种操作，提高哈希函数的质量
- 最后检查代码，发现有的地方循环体里面有函数调用`for(int i = 0; i < a.size(); i++)`

- 注意：如果代码对于输入比较敏感，只针对某一个输入优化，对于另外的输入可能适得其反
